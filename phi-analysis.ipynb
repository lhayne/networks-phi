{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyphi\n",
    "from itertools import chain,combinations\n",
    "from scipy import stats\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class phi():\n",
    "    def __init__(self,tpm,states=None):\n",
    "        self.tpm = tpm\n",
    "        self.num_nodes = int(np.log2(len(tpm)))\n",
    "        if states is None:\n",
    "            self.states = np.zeros((self.num_nodes))\n",
    "        else:\n",
    "            self.states = states\n",
    "        \n",
    "    # Converts a list of states to a row number, so that we can index the tpm\n",
    "    # input : list of states, Ex. [0,1,0,0]\n",
    "    # output : row number based on little endian notation, so index zero in list is least\n",
    "    #          significant bit and index n-1 is most significant bit. Ex. [0,1,0,0] -> 2\n",
    "    def state_to_index(self,states):\n",
    "        decimal = 0\n",
    "        for i,state in enumerate(states):\n",
    "            decimal += (2**i) * (state)\n",
    "        return decimal\n",
    "    \n",
    "    # Converts a row number from the tpm into a list of states\n",
    "    # input : row number, total number of states. Ex. (2, 4)\n",
    "    # output : list of states that row number represents. Ex. (2, 4) -> [0,1,0,0]\n",
    "    #          We include the number of states so that we know how many zeros to append at the end.\n",
    "    def index_to_state(self,index,num_states):\n",
    "        binary = bin(index)[2:]\n",
    "        states = []\n",
    "        for state in binary: # convert row to binary\n",
    "            states.append(int(state))\n",
    "        \n",
    "        states = np.flip(states) # flip to make little endian\n",
    "       \n",
    "        if (len(states) < num_states): \n",
    "            states = np.concatenate((states,np.zeros((num_states-len(states)),dtype=int)))\n",
    "            \n",
    "        return states\n",
    "    \n",
    "    # Because they're conditional probability distributions, the rows need to sum to one\n",
    "    def normalize_rows(self,tpm):\n",
    "        return tpm/np.sum(tpm,1)[:, np.newaxis]\n",
    "    \n",
    "    # return the powerset of a list of nodes\n",
    "    def powerset(self,iterable):\n",
    "        \"powerset([1,2,3]) --> (1,) (2,) (3,) (1,2) (1,3) (2,3)\"\n",
    "        s = list(iterable)\n",
    "        return chain.from_iterable(combinations(s, r) for r in range(1,len(s)))\n",
    "    \n",
    "    # return the powerset of a list of nodes including full set\n",
    "    def powerset_all(self,iterable):\n",
    "        \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "        s = list(iterable)\n",
    "        return chain.from_iterable(combinations(s, r) for r in range(0,len(s)+1))\n",
    "    \n",
    "    # returns the tensor product of two tpm (effect)\n",
    "    def tensor_product(self,t1,t2):\n",
    "        tensor_product = np.zeros((t1.shape[0],t1.shape[1]*t2.shape[1]))\n",
    "        column = 0\n",
    "        for c2 in t2.T:\n",
    "            for c1 in t1.T:\n",
    "                tensor_product[:,column] = c1*c2\n",
    "                column+=1\n",
    "                \n",
    "        return tensor_product\n",
    "    \n",
    "    # This tensor product combines two effect repertoires t1 and t2, given a list of their nodes also\n",
    "    # This ensures that the nodes get ordered correctly even if the purview nodes of t1 are 'B' and the\n",
    "    # purview nodes of t2 are 'A,C'.\n",
    "    def tensor_product_ordered(self,t1,t1_nodes,t2,t2_nodes):\n",
    "        tensor_product = np.zeros((t1.shape[0],t1.shape[1]*t2.shape[1]))\n",
    "        \n",
    "        # when we perform the tensor product we want to put the effect nodes in the correct order in the\n",
    "        # new tpm, but the purview might not be the full system of nodes, so we create a sorted list\n",
    "        # and index each purview node in the list\n",
    "        sorted_nodes = np.sort(t1_nodes + t2_nodes)\n",
    "        ordered_indexes = np.zeros((self.num_nodes),dtype=int)\n",
    "        order = 0\n",
    "        \n",
    "        for node in sorted_nodes:\n",
    "            ordered_indexes[node] = order\n",
    "            order+=1\n",
    "\n",
    "        columns = tensor_product.shape[1]\n",
    "        \n",
    "        # Fill the columns of the tpm one at a time\n",
    "        for column in range(columns):\n",
    "            # find the states of t1's nodes given the column we're currently filling\n",
    "            t1_state = self.index_to_state(column,int(np.log2(columns)))[ordered_indexes[t1_nodes]]\n",
    "            # do the same for t2's nodes\n",
    "            t2_state = self.index_to_state(column,int(np.log2(columns)))[ordered_indexes[t2_nodes]]\n",
    "            # Then fill the column with the correct column from t1 multiplied by the correct column from\n",
    "            # t2\n",
    "            tensor_product[:,column] = t1[:,self.state_to_index(t1_state)] * t2[:,self.state_to_index(t2_state)]\n",
    "            \n",
    "        return tensor_product\n",
    "        \n",
    "    \"\"\"\n",
    "    effect_repertoire :\n",
    "        input : mechanism (t nodes), purview (t+1 nodes)\n",
    "        algorithm : from original tpm find the tpm P(purview | mechanism)\n",
    "        output : repertoire for those nodes\n",
    "    \"\"\"    \n",
    "    def effect_repertoire(self,mechanism,purview):\n",
    "        # The effect repertoire captures the conditional transition probability of transitioning to each\n",
    "        # purview state, given the current mechanism state, so it needs to be of size\n",
    "        # NUMBER POSSIBLE MECHANISM STATES X NUMBER POSSIBLE PURVIEW STATES, but we start by marginalizing\n",
    "        # over only the mechanism states, so NUMBER POSSIBLE MECHANISM STATES X NUMBER POSSIBLE STATES\n",
    "        mechanism_effect_repertoire = np.zeros((2**len(mechanism),2**self.num_nodes))\n",
    "        \n",
    "        # We marginalize over the mechanism states, which means we sum the probabilities of rows which\n",
    "        # only differ in the state of nodes not in the mechanism.\n",
    "        # We do this by finding the mechanism's state for a given row and mapping that row in the original tpm\n",
    "        # to the correct row in the new mechanism repertoire\n",
    "        for row in range(self.tpm.shape[0]):\n",
    "            mechanism_state = self.index_to_state(row,self.num_nodes)[mechanism]\n",
    "            mechanism_effect_repertoire[self.state_to_index(mechanism_state),:] += self.tpm[row,:]\n",
    "        \n",
    "        # This is the final effect repertoire\n",
    "        effect_repertoire = np.zeros((2**len(mechanism),2**len(purview)))\n",
    "        \n",
    "        # Second, we marginalize over the column states, which means we sum the probabilities of columns which\n",
    "        # only differ in the state of nodes not in the purview.\n",
    "        # We do this by finding the purview's state for a given column and mapping that row in the original tpm\n",
    "        # to the correct column in the new effect repertoire\n",
    "        for column in range(self.tpm.shape[1]):\n",
    "            purview_state = self.index_to_state(column,self.num_nodes)[purview]\n",
    "            effect_repertoire[:,self.state_to_index(purview_state)] += mechanism_effect_repertoire[:,column]\n",
    "        \n",
    "        # All that's left to do is normalize the rows because each row is a conditional probability distribution\n",
    "        effect_repertoire = self.normalize_rows(effect_repertoire)\n",
    "        \n",
    "        # Now, we have to expand the effect_repertoire into the original state space which has all the \n",
    "        # possible current states at time t\n",
    "        expanded_effect_repertoire = np.zeros((2**self.num_nodes,2**len(purview)))\n",
    "        \n",
    "        # This is done by mapping distributions in the effect_repertoire to each row in the expanded repertoire\n",
    "        # where the mechanism's state matches\n",
    "        for row in range(2**self.num_nodes):\n",
    "            mechanism_state = self.index_to_state(row,self.num_nodes)[mechanism]\n",
    "            expanded_effect_repertoire[row,:] = effect_repertoire[self.state_to_index(mechanism_state),:]\n",
    "            \n",
    "        return expanded_effect_repertoire\n",
    "    \n",
    "    def effect_mip(self,system,purview):        \n",
    "        # We need to find the cut the makes the least difference to the tpm of the system,\n",
    "        # so we generate the original tpm first. We'll want to compare the original tpm\n",
    "        # with the tpm generated after making our cut to determine the cut that makes the\n",
    "        # least difference.\n",
    "        min_cut = None\n",
    "        cut_distance = float('inf')\n",
    "        full_tpm = self.effect_repertoire(system,purview)\n",
    "        \n",
    "        # We need to loop through two powersets to determine all the cuts for this particular \n",
    "        # system + purview pair. We need to loop through the system powerset and the purview powerset\n",
    "        # See factors.png below for an example.\n",
    "        \n",
    "        seen = set()\n",
    "        \n",
    "        for system_subset in self.powerset_all(system):\n",
    "            \n",
    "            for purview_subset in self.powerset_all(purview):\n",
    "                \n",
    "                # We haven't already seen the complement of the subset and the length of both of the factors\n",
    "                # aren't zero\n",
    "                if system_subset not in seen and not (len(system_subset) == 0 and len(purview_subset) == 0):\n",
    "                    system_factor_1 = list(system_subset)\n",
    "                    purview_factor_1 = list(purview_subset)\n",
    "                    \n",
    "                    system_factor_2 = list(np.setdiff1d(system,system_subset))\n",
    "                    purview_factor_2 = list(np.setdiff1d(purview,purview_subset))\n",
    "                    \n",
    "                    tpm_1 = self.effect_repertoire(system_factor_1,purview_factor_1)\n",
    "                    tpm_2 = self.effect_repertoire(system_factor_2,purview_factor_2)\n",
    "                    \n",
    "                    cut_tpm = self.tensor_product_ordered(tpm_1,purview_factor_1,tpm_2,purview_factor_2)\n",
    "                    \n",
    "                    # assess the distance between the cut_tpm and the full_tpm\n",
    "                    new_cut_distance = stats.wasserstein_distance(full_tpm[self.state_to_index(self.states)],\n",
    "                                                         cut_tpm[self.state_to_index(self.states)])\n",
    "                    \n",
    "#                     print ((system_factor_1,purview_factor_1,system_factor_2,purview_factor_2),\" \",new_cut_distance)\n",
    "                    \n",
    "                    # update the distance with the lowest distance\n",
    "                    if new_cut_distance <= cut_distance:\n",
    "                        cut_distance = new_cut_distance\n",
    "                        min_cut = (system_factor_1,purview_factor_1,system_factor_2,purview_factor_2)\n",
    "            \n",
    "            # add the complement of the system_subset to the seen set to ensure we don't double count certain\n",
    "            # partitions\n",
    "            seen.add(tuple(np.setdiff1d(system,system_subset)))\n",
    "        \n",
    "        return cut_distance,min_cut\n",
    "    \n",
    "    # TODO: implement this. Right now it's not working when the purview is smaller than the full\n",
    "    # list of nodes because the tensor product assumes the second dimension will be the full length\n",
    "    # of all the states given all the nodes\n",
    "    def mie(self,system):\n",
    "        phi = float('-inf')\n",
    "        mie = None\n",
    "        for purview in self.powerset_all(np.arange(self.num_nodes)):\n",
    "            cut_distance,min_cut = self.effect_mip(system,list(purview))\n",
    "            if cut_distance > phi:\n",
    "                phi = cut_distance\n",
    "                mie = purview\n",
    "        return phi,mie\n",
    "    \n",
    "    def concept(self,system):\n",
    "        \n",
    "        effect_phi,mie = self.mie(system)\n",
    "        cause_phi,mic = self.mic(system) # to be implemented\n",
    "        \n",
    "        return (min(effect_phi,cause_phi),mic,mie)\n",
    "                        \n",
    "    \"\"\"    \n",
    "    concept :\n",
    "        input : list of nodes in system (in our case this will be the whole system)\n",
    "        \n",
    "        algorithm : min (mic(system),mie(system))\n",
    "        \n",
    "        output : phi value for that system\n",
    "    \"\"\"\n",
    "\n",
    "    def cause_repertoire(self,purview,mechanism):\n",
    "        # The cause repertoire captures the conditional transition probability of transitioning to each\n",
    "        # purview state, given the current mechanism state, so it needs to be of size\n",
    "        # NUMBER POSSIBLE MECHANISM STATES X NUMBER POSSIBLE PURVIEW STATES, but we start by marginalizing\n",
    "        # over only the mechanism states, so NUMBER POSSIBLE MECHANISM STATES X NUMBER POSSIBLE STATES\n",
    "        mechanism_cause_repertoire = np.zeros((2**len(mechanism),2**self.num_nodes))\n",
    "        \n",
    "        # We marginalize over the mechanism states, which means we sum the probabilities of rows which\n",
    "        # only differ in the state of nodes not in the mechanism.\n",
    "        # We do this by finding the mechanism's state for a given row and mapping that row in the original tpm\n",
    "        # to the correct row in the new mechanism repertoire\n",
    "        for row in range(self.tpm.shape[0]):\n",
    "            mechanism_state = self.index_to_state(row,self.num_nodes)[mechanism]\n",
    "            mechanism_cause_repertoire[self.state_to_index(mechanism_state),:] += self.tpm[row,:]\n",
    "        \n",
    "        # This is the final cause repertoire\n",
    "        cause_repertoire = np.zeros((2**len(mechanism),2**len(purview)))\n",
    "        \n",
    "        # Second, we marginalize over the column states, which means we sum the probabilities of columns which\n",
    "        # only differ in the state of nodes not in the purview.\n",
    "        # We do this by finding the purview's state for a given column and mapping that row in the original tpm\n",
    "        # to the correct column in the new cause repertoire\n",
    "        for column in range(self.tpm.shape[1]):\n",
    "            purview_state = self.index_to_state(column,self.num_nodes)[purview]\n",
    "            cause_repertoire[:,self.state_to_index(purview_state)] += mechanism_cause_repertoire[:,column]\n",
    "        \n",
    "        # All that's left to do is normalize the rows because each row is a conditional probability distribution\n",
    "        cause_repertoire = self.normalize_rows(cause_repertoire)\n",
    "        \n",
    "        # Now, we have to expand the cause_repertoire into the original state space which has all the \n",
    "        # possible current states at time t\n",
    "        expanded_cause_repertoire = np.zeros((2**self.num_nodes,2**len(purview)))\n",
    "        \n",
    "        # This is done by mapping distributions in the cause_repertoire to each row in the expanded repertoire\n",
    "        # where the mechanism's state matches\n",
    "        for row in range(2**self.num_nodes):\n",
    "            mechanism_state = self.index_to_state(row,self.num_nodes)[mechanism]\n",
    "            expanded_cause_repertoire[row,:] = cause_repertoire[self.state_to_index(mechanism_state),:]\n",
    "            \n",
    "        # this ASSUMES there is only 1 thing in the purview, not sure what to do if there are more...\n",
    "        # drop the column that are not active in purview\n",
    "        # if len(purview) > 0:\n",
    "        #     return expanded_cause_repertoire[:,self.states[purview[0]]]\n",
    "        # else:\n",
    "        #     return expanded_cause_repertoire\n",
    "        return expanded_cause_repertoire\n",
    "    \n",
    "    def cause_tensor_product(self,t1,t2,purview):\n",
    "        excluded_from_purview = set(range(len(starting_states))) - set(purview)\n",
    "        # multiply possibilities of cause_repertoire and things not in cause_repertoire\n",
    "        expanded_cause_repertoire_unsorted = np.array([])\n",
    "\n",
    "        for i in range(len(t2)):\n",
    "            expanded_cause_repertoire_unsorted = np.append(expanded_cause_repertoire_unsorted,t1 * t2[i])\n",
    "        \n",
    "        # make a copy of it\n",
    "        expanded_cause_repertoire = np.zeros((len(expanded_cause_repertoire_unsorted)))\n",
    "\n",
    "        # sort the copy\n",
    "        ordered = list(purview) + list(excluded_from_purview)\n",
    "        for i in range(len(expanded_cause_repertoire_unsorted)):\n",
    "            # make binary array\n",
    "            s = self.index_to_state(i,len(starting_states))\n",
    "            #re-order digits\n",
    "            reordered_s = np.full(len(s),9)\n",
    "            for j in range(len(ordered)):\n",
    "                reordered_s[j] = s[ordered[j]]\n",
    "            index = self.state_to_index(reordered_s)\n",
    "            expanded_cause_repertoire[i] = expanded_cause_repertoire_unsorted[index]\n",
    "        return expanded_cause_repertoire\n",
    "\n",
    "    def cause_mip(self,system,purview):        \n",
    "        # We need to find the cut the makes the least difference to the tpm of the system,\n",
    "        # so we generate the original tpm first. We'll want to compare the original tpm\n",
    "        # with the tpm generated after making our cut to determine the cut that makes the\n",
    "        # least difference.\n",
    "        min_cut = None\n",
    "        cut_distance = float('inf')\n",
    "        full_tpm = self.cause_repertoire(system,purview)\n",
    "        \n",
    "        # We need to loop through two powersets to determine all the cuts for this particular \n",
    "        # system + purview pair. We need to loop through the system powerset and the purview powerset\n",
    "        # See factors.png below for an example.\n",
    "        \n",
    "        seen = set()\n",
    "        \n",
    "        for system_subset in self.powerset_all(system):\n",
    "            \n",
    "            for purview_subset in self.powerset_all(purview):\n",
    "                \n",
    "                # We haven't already seen the complement of the subset and the length of both of the factors\n",
    "                # aren't zero\n",
    "                if system_subset not in seen and not (len(system_subset) == 0 and len(purview_subset) == 0):\n",
    "                    system_factor_1 = list(system_subset)\n",
    "                    purview_factor_1 = list(purview_subset)\n",
    "                    \n",
    "                    system_factor_2 = list(np.setdiff1d(system,system_subset))\n",
    "                    purview_factor_2 = list(np.setdiff1d(purview,purview_subset))\n",
    "                    \n",
    "                    tpm_1 = self.cause_repertoire(system_factor_1,purview_factor_1)\n",
    "                    tpm_2 = self.cause_repertoire(system_factor_2,purview_factor_2)\n",
    "                    \n",
    "                    cut_tpm = self.tensor_product_ordered(tpm_1,system_factor_1,tpm_2,system_factor_2)\n",
    "                    # assess the distance between the cut_tpm and the full_tpm\n",
    "                    new_cut_distance = stats.wasserstein_distance(full_tpm[self.state_to_index(self.states)],\n",
    "                                                         cut_tpm[self.state_to_index(self.states)])\n",
    "                    \n",
    "#                     print ((system_factor_1,purview_factor_1,system_factor_2,purview_factor_2),\" \",new_cut_distance)\n",
    "                    \n",
    "                    # update the distance with the lowest distance\n",
    "                    if new_cut_distance <= cut_distance:\n",
    "                        cut_distance = new_cut_distance\n",
    "                        min_cut = (system_factor_1,purview_factor_1,system_factor_2,purview_factor_2)\n",
    "            \n",
    "            # add the complement of the system_subset to the seen set to ensure we don't double count certain\n",
    "            # partitions\n",
    "            seen.add(tuple(np.setdiff1d(system,system_subset)))\n",
    "        \n",
    "        return cut_distance,min_cut\n",
    "\n",
    "    def mic(self,system):\n",
    "        phi = float('-inf')\n",
    "        mic = None\n",
    "        for purview in self.powerset_all(np.arange(self.num_nodes)):\n",
    "            cut_distance,min_cut = self.cause_mip(system,list(purview))\n",
    "            if cut_distance >= phi:\n",
    "                phi = cut_distance\n",
    "                mic = purview\n",
    "        return phi,mic\n",
    "    \n",
    "    def get_phi(self,system):\n",
    "        phi = 0\n",
    "        for s in all_subsets(system):\n",
    "            p = self.concept(list(set(s)))\n",
    "            if p[0] > phi and p[0] != float(\"inf\"):\n",
    "                phi = p[0]\n",
    "        return phi\n",
    "            \n",
    "def all_subsets(ss):\n",
    "    return chain(*map(lambda x: combinations(ss, x), range(0, len(ss)+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the files but exclude the row and column numbers\n",
    "sleep = np.genfromtxt('sleep_mat/s4_sleep_t.csv', delimiter=',')[1:,1:]\n",
    "wake = np.genfromtxt('sleep_mat/s4_wake_t.csv', delimiter=',')[1:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_state(index,num_states):\n",
    "        binary = bin(index)[2:]\n",
    "        states = []\n",
    "        for state in binary: # convert row to binary\n",
    "            states.append(int(state))\n",
    "        \n",
    "        states = np.flip(states) # flip to make little endian\n",
    "       \n",
    "        if (len(states) < num_states): \n",
    "            states = np.concatenate((states,np.zeros((num_states-len(states)),dtype=int)))\n",
    "            \n",
    "        return states\n",
    "\n",
    "def pick_state(tpm1, tpm2):\n",
    "    n = int(np.log2(tpm1.shape[0]))\n",
    "    states_on = np.zeros((n))\n",
    "    states_off = np.zeros((n))\n",
    "    for i in range(tpm1.shape[0]):\n",
    "        bi = index_to_state(i,n)\n",
    "        total = np.sum(tpm1[:,i])\n",
    "        for j in range(n):\n",
    "            if bi[j] == 0:\n",
    "                states_off[j] += total\n",
    "            else:\n",
    "                states_on[j] += total\n",
    "    majority_states = []\n",
    "    for i in range(n):\n",
    "        if states_off[i] > states_on[i]:\n",
    "            majority_states.append(0)\n",
    "        else:\n",
    "            majority_states.append(1)\n",
    "    return majority_states\n",
    "\n",
    "def pick_state2(tpm1, tpm2):\n",
    "    n = int(np.log2(tpm1.shape[0]))\n",
    "    max_i = 0\n",
    "    max_s = 0\n",
    "    for i in range(1,tpm1.shape[0]):\n",
    "        s = np.sum(tpm1[:,i]) + np.sum(tpm2[:,i])\n",
    "        if s > max_s:\n",
    "            max_s = s\n",
    "            max_i = i\n",
    "    return index_to_state(max_i,n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a starting state - most common state across both TPMs\n",
    "# is there a signficicance difference? between sleep and awake\n",
    "# how does phi change on average as the number of nodes (channels) increases?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting states: [0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michael/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting states: [0 0 0]\n",
      "starting states: [0 0 0]\n",
      "starting states: [0 0 0]\n",
      "starting states: [0 0 0]\n",
      "starting states: [0 0 0]\n",
      "starting states: [0 0 0]\n",
      "starting states: [0 0 0]\n",
      "starting states: [0 0 0]\n",
      "starting states: [0 0 0]\n",
      "starting states: [0 0 0]\n",
      "starting states: [0 0 0]\n",
      "starting states: [0 0 0]\n",
      "starting states: [0 0 0]\n",
      "starting states: [0 0 0]\n",
      "starting states: [1 0 0]\n",
      "starting states: [1 0 0]\n",
      "starting states: [1 0 0]\n",
      "starting states: [1 0 0]\n",
      "starting states: [1 0 0]\n",
      "starting states: [1 0 0]\n",
      "starting states: [1 0 0]\n",
      "starting states: [1 0 0]\n",
      "starting states: [1 0 0]\n",
      "starting states: [1 0 0]\n",
      "starting states: [1 0 0]\n",
      "starting states: [1 0 0]\n",
      "starting states: [1 0 0]\n",
      "starting states: [1 0 0]\n",
      "starting states: [1 0 0]\n",
      "starting states: [0 1 0]\n",
      "starting states: [0 1 0]\n",
      "starting states: [0 1 0]\n",
      "starting states: [0 1 0]\n",
      "starting states: [0 1 0]\n",
      "starting states: [0 1 0]\n",
      "starting states: [0 1 0]\n",
      "starting states: [0 1 0]\n",
      "starting states: [0 1 0]\n",
      "starting states: [0 1 0]\n",
      "starting states: [0 1 0]\n",
      "starting states: [0 1 0]\n",
      "starting states: [0 1 0]\n",
      "starting states: [0 1 0]\n",
      "starting states: [0 1 0]\n",
      "starting states: [1 1 0]\n",
      "starting states: [1 1 0]\n",
      "starting states: [1 1 0]\n",
      "starting states: [1 1 0]\n",
      "starting states: [1 1 0]\n",
      "starting states: [1 1 0]\n",
      "starting states: [1 1 0]\n",
      "starting states: [1 1 0]\n",
      "starting states: [1 1 0]\n",
      "starting states: [1 1 0]\n",
      "starting states: [1 1 0]\n",
      "starting states: [1 1 0]\n",
      "starting states: [1 1 0]\n",
      "starting states: [1 1 0]\n",
      "starting states: [1 1 0]\n",
      "starting states: [0 0 1]\n",
      "starting states: [0 0 1]\n",
      "starting states: [0 0 1]\n",
      "starting states: [0 0 1]\n",
      "starting states: [0 0 1]\n",
      "starting states: [0 0 1]\n",
      "starting states: [0 0 1]\n",
      "starting states: [0 0 1]\n",
      "starting states: [0 0 1]\n",
      "starting states: [0 0 1]\n",
      "starting states: [0 0 1]\n",
      "starting states: [0 0 1]\n",
      "starting states: [0 0 1]\n",
      "starting states: [0 0 1]\n",
      "starting states: [0 0 1]\n",
      "starting states: [1 0 1]\n",
      "starting states: [1 0 1]\n",
      "starting states: [1 0 1]\n",
      "starting states: [1 0 1]\n",
      "starting states: [1 0 1]\n",
      "starting states: [1 0 1]\n",
      "starting states: [1 0 1]\n",
      "starting states: [1 0 1]\n",
      "starting states: [1 0 1]\n",
      "starting states: [1 0 1]\n",
      "starting states: [1 0 1]\n",
      "starting states: [1 0 1]\n",
      "starting states: [1 0 1]\n",
      "starting states: [1 0 1]\n",
      "starting states: [1 0 1]\n",
      "starting states: [0 1 1]\n",
      "starting states: [0 1 1]\n",
      "starting states: [0 1 1]\n",
      "starting states: [0 1 1]\n",
      "starting states: [0 1 1]\n",
      "starting states: [0 1 1]\n",
      "starting states: [0 1 1]\n",
      "starting states: [0 1 1]\n",
      "starting states: [0 1 1]\n",
      "starting states: [0 1 1]\n",
      "starting states: [0 1 1]\n",
      "starting states: [0 1 1]\n",
      "starting states: [0 1 1]\n",
      "starting states: [0 1 1]\n",
      "starting states: [0 1 1]\n",
      "starting states: [1 1 1]\n",
      "starting states: [1 1 1]\n",
      "starting states: [1 1 1]\n",
      "starting states: [1 1 1]\n",
      "starting states: [1 1 1]\n",
      "starting states: [1 1 1]\n",
      "starting states: [1 1 1]\n",
      "starting states: [1 1 1]\n",
      "starting states: [1 1 1]\n",
      "starting states: [1 1 1]\n",
      "starting states: [1 1 1]\n",
      "starting states: [1 1 1]\n",
      "starting states: [1 1 1]\n",
      "starting states: [1 1 1]\n",
      "starting states: [1 1 1]\n",
      "starting states: [0 0 0 1]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 8 is out of bounds for axis 0 with size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-32b28ee4e82b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m       'sleep_mat/s17_wake_t.csv','sleep_mat/s18_wake_t.csv',]\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# all_phis(sl,wl,'nodes3-states2.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mall_phis_for_all_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nodes3-all-states.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;31m# print(get_phis('s3_sleep_t.csv','s3_wake_t.csv'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# print(get_phis('sleep_mat/s4_sleep_t.csv','sleep_mat/s4_wake_t.csv'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-32b28ee4e82b>\u001b[0m in \u001b[0;36mall_phis_for_all_states\u001b[0;34m(sleep_list, wake_list, out_file_name, n)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_to_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_phis_for_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwake_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresults_count\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mresults_count\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-32b28ee4e82b>\u001b[0m in \u001b[0;36mget_phis_for_state\u001b[0;34m(sleep_file, wake_file, starting_state)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msleep_phi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstarting_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mwake_phi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwake\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstarting_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msleep_phi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_phi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mwp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwake_phi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_phi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-a82fb860e50b>\u001b[0m in \u001b[0;36mget_phi\u001b[0;34m(self, system)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_subsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mphi\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-a82fb860e50b>\u001b[0m in \u001b[0;36mconcept\u001b[0;34m(self, system)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconcept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0meffect_phi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0mcause_phi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# to be implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-a82fb860e50b>\u001b[0m in \u001b[0;36mmie\u001b[0;34m(self, system)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mmie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpurview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpowerset_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mcut_distance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_cut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffect_mip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpurview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcut_distance\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcut_distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-a82fb860e50b>\u001b[0m in \u001b[0;36meffect_mip\u001b[0;34m(self, system, purview)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0;31m# assess the distance between the cut_tpm and the full_tpm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                     new_cut_distance = stats.wasserstein_distance(full_tpm[self.state_to_index(self.states)],\n\u001b[0m\u001b[1;32m    177\u001b[0m                                                          cut_tpm[self.state_to_index(self.states)])\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 8 is out of bounds for axis 0 with size 8"
     ]
    }
   ],
   "source": [
    "def get_phis(sleep_file,wake_file):\n",
    "    sleep = np.genfromtxt(sleep_file, delimiter=',')[1:,1:]\n",
    "    wake = np.genfromtxt(wake_file, delimiter=',')[1:,1:]\n",
    "    starting_state = pick_state2(sleep,wake)\n",
    "    print('starting states: ' + str(starting_state))\n",
    "    sleep_phi = phi(sleep,starting_state)\n",
    "    wake_phi = phi(wake,starting_state)\n",
    "    sp = sleep_phi.get_phi([0,1,2])\n",
    "    wp = wake_phi.get_phi([0,1,2])\n",
    "    return sp,wp\n",
    "\n",
    "def get_phis_for_state(sleep_file,wake_file,starting_state):\n",
    "    sleep = np.genfromtxt(sleep_file, delimiter=',')[1:,1:]\n",
    "    wake = np.genfromtxt(wake_file, delimiter=',')[1:,1:]\n",
    "    print('starting states: ' + str(starting_state))\n",
    "    sleep_phi = phi(sleep,starting_state)\n",
    "    wake_phi = phi(wake,starting_state)\n",
    "    sp = sleep_phi.get_phi([0,1,2])\n",
    "    wp = wake_phi.get_phi([0,1,2])\n",
    "    return sp,wp\n",
    "\n",
    "def all_phis(sleep_list,wake_list,out_file_name):\n",
    "    results = pd.DataFrame({'sleep':[],'awake':[]})\n",
    "    results_count = 0\n",
    "    for i in range(len(sleep_list)):\n",
    "        sp,wp = get_phis(sleep_list[i],wake_list[i])\n",
    "        results.loc[i] = [sp,wp]\n",
    "    results.to_csv(out_file_name)\n",
    "    \n",
    "def all_phis_for_all_states(sleep_list,wake_list,out_file_name,n):\n",
    "    results = pd.DataFrame({'sleep':[],'awake':[],'states':[]})\n",
    "    results_count = 0\n",
    "    results.to_csv(out_file_name)\n",
    "    for j in range(n*n):\n",
    "        state = index_to_state(j,n)\n",
    "        for i in range(len(sleep_list)):\n",
    "            sp,wp = get_phis_for_state(sleep_list[i],wake_list[i],state)\n",
    "            results.loc[results_count] = [sp,wp,str(state)]\n",
    "            results_count+=1\n",
    "        results.to_csv(out_file_name, mode='a', header=False)\n",
    "sl = ['sleep_mat/s3_sleep_t.csv','sleep_mat/s4_sleep_t.csv',\n",
    "      'sleep_mat/s5_sleep_t.csv','sleep_mat/s6_sleep_t.csv',\n",
    "      'sleep_mat/s7_sleep_t.csv','sleep_mat/s8_sleep_t.csv',\n",
    "      'sleep_mat/s9_sleep_t.csv','sleep_mat/s10_sleep_t.csv',\n",
    "      'sleep_mat/s11_sleep_t.csv','sleep_mat/s12_sleep_t.csv',\n",
    "      'sleep_mat/s13_sleep_t.csv',\n",
    "      'sleep_mat/s15_sleep_t.csv','sleep_mat/s16_sleep_t.csv',\n",
    "      'sleep_mat/s17_sleep_t.csv','sleep_mat/s18_wake_t.csv',]\n",
    "wl = ['sleep_mat/s3_wake_t.csv','sleep_mat/s4_wake_t.csv',\n",
    "      'sleep_mat/s5_wake_t.csv','sleep_mat/s6_wake_t.csv',\n",
    "      'sleep_mat/s7_wake_t.csv','sleep_mat/s8_wake_t.csv',\n",
    "      'sleep_mat/s9_wake_t.csv','sleep_mat/s10_wake_t.csv',\n",
    "      'sleep_mat/s11_wake_t.csv','sleep_mat/s12_wake_t.csv',\n",
    "      'sleep_mat/s13_wake_t.csv',\n",
    "      'sleep_mat/s15_wake_t.csv','sleep_mat/s16_wake_t.csv',\n",
    "      'sleep_mat/s17_wake_t.csv','sleep_mat/s18_wake_t.csv',]\n",
    "# all_phis(sl,wl,'nodes3-states2.csv')\n",
    "all_phis_for_all_states(sl,wl,'nodes3-all-states.csv',3)\n",
    "# print(get_phis('s3_sleep_t.csv','s3_wake_t.csv'))\n",
    "# print(get_phis('sleep_mat/s4_sleep_t.csv','sleep_mat/s4_wake_t.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
